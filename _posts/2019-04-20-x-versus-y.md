---
layout: post
title: "X vs Y"
author: "Sonya B"
categories: journal
tags: [r, linear regression]
image: arrows.jpg
usemathjax: true
---

In this post we'll address the question

    what happens when I switch the roles of X and Y in a linear regression?

Specifically we'll investigate what happens to the coefficients in a linear regression under an exchange of $$X$$ and $$Y$$. (In this post we are ignoring the causal interpretations of the variables and thinking just about the math.)

First we'll generate some data using a linear relationship between random variables $$X$$ and $$Y$$ and with normally distributed, mean 0 and constant variance errors regarding the measurement of $$Y$$ (so that it is reasonable to use ordinary least squares linear regression). We'll set up $$X$$ so that it's also normally distributed.

Let's generate some data and plot it.

```
library(tidyverse)
set.seed(1020)

# parameters for distribution of X
mean_x = 1
var_x = 0.04

# linear effect x on y
alpha <- 2
var_epsilon <- 1

# n data points
n_data <- 250

df <- data.frame(x = rnorm(n=n_data, mean=mean_x, sd=sqrt(var_x))) %>%
  mutate(y = alpha*x + rnorm(n = n_data, sd = sqrt(var_epsilon)))

df %>%
  ggplot(aes(x = x, y = y)) +
  geom_point(color = "blue") +
  expand_limits(x = 0, y = 0)
```

![](https://meticulousdatascience.com/assets/img/x_versus_y_data.png)

We have set things up so that, up to measurement error of $$y$$, $$y = 2x$$. It's important to note that it's assumed there's no measurement error in $$x$$ when we build our linear regression ($$y$$ takes all the error), which is how we set up our data. Not surprisingly, when we build our linear model we get a slope close to 2 and an intercept close to 0.

```
> mod_1 <- df %>%
+   lm(formula = y ~ x)
> summary(mod_1)

Call:
lm(formula = y ~ x, data = .)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.74944 -0.62306 -0.05544  0.69974  2.62335 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -0.002259   0.341678  -0.007    0.995    
x            2.031379   0.336174   6.043 5.52e-09 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.012 on 248 degrees of freedom
Multiple R-squared:  0.1283,  Adjusted R-squared:  0.1248 
F-statistic: 36.51 on 1 and 248 DF,  p-value: 5.524e-09
```

The regression coefficient for $x$ in this model is given by

$$r_{yx} = \rho_{xy} \frac{\sigma_x}{\sigma_y}$$

where $$\rho_{xy}$$ is the correlation between $$x$$ and $$y$$ (which can be further decomposed into covariance and variance terms, but for the purposes of this post we just want to notice that it's symmetric, i.e. $$\rho_{xy} = \rho_{yx}$$). 

Now, if we just go off the intuition we get from algebra, then we would expect because $$y = 2x$$, you can solve for $$x$$ and obtain $$x = 1/2 y$$. But, confusingly, when we do the "reverse" regression we do not get a slope near 1/2, nor an intercept near 0, as seen below. What is going on here?

```
> mod_2 <- df %>%
+   lm(formula = x ~ y)
> summary(mod_2)

Call:
lm(formula = x ~ y, data = .)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.42129 -0.13753  0.00579  0.12831  0.43991 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.87040    0.02400  36.268  < 2e-16 ***
y            0.06318    0.01046   6.043 5.52e-09 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1784 on 248 degrees of freedom
Multiple R-squared:  0.1283,  Adjusted R-squared:  0.1248 
F-statistic: 36.51 on 1 and 248 DF,  p-value: 5.524e-09
```

There are several reasons why we can see mathematically that the coefficient we get when we regress X on Y should not be 1/2. The easiest way to see this is that the coefficient should follow the formula

$$r_{xy} = \rho_{yx} \frac{\sigma_y}{\sigma_x}$$

which is certainly not the multiplicative inverse of $$r_{yx}$$ because $$\rho_{xy} = \rho_{yx}$$. 

One of the key differences between regressing Y on X versus regressing X on Y is the assumption we make about the errors. As mentioned, when we regress Y on X, we assume all the measurement error lives in $$y$$ whereas when we regress X on Y we assume the measurement errors live in $$x$$, which is not how we set up the problem. This change in assumption about where the errors live means we are minimizing a different loss function when we fit the line, and this change does not manifest in the way we would expect from the algebra, i.e. by inverting the coefficient.

Another appealing explanation is the observation that when we regress Y on X we are predicting the mean value of $y$ we expect to obtain for a given $$x$$. Rearranging the equation algebraically does not get us to a situation where we are instead predicting the mean value of $$x$$ given a value of $$y$$, which is what the regression coefficients define.

The take-home message is that equations you get from applying statistical techniques to data have interpretations that are more complicated than simple algebraic rules can encode.
